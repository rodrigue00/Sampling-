import numpy as np
import pandas as pd
import json
import optuna
import pickle
import os
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, f1_score
from xgboost import XGBClassifier
from sklearn.mixture import GaussianMixture

# Charger la configuration depuis config.json
with open("config.json", "r") as config_file:
    config = json.load(config_file)

file_path = config["file_path"]
model_save_path = config["model_save_path"]

# Charger le dataset CSV avec gestion des erreurs
df = pd.read_csv(file_path)
print(" Donn√©es charg√©es avec succ√®s.")

# D√©finition de la colonne cible
target_column = config["target_column"]
if target_column not in df.columns:
    raise ValueError(f" La colonne cible '{target_column}' est absente des donn√©es.")

print(f" Colonne cible d√©finie : {target_column}")

# Fonction de sampling avec GMM
# Utilisation des param√®tres d√©finis dans config.json
def gmm_sampling(df, target_column, config):
    n_components_range = config['gmm']['n_components_range']
    covariance_types = config['gmm']['covariance_types']
    reduction_rate = config['sampling']['fraction']

    # S√©paration des features et de la cible
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Calcul du nombre total de points √† g√©n√©rer
    total_samples = int(len(X) * reduction_rate)

    # Fonction pour optimiser le GMM et g√©n√©rer des √©chantillons
    def generate_samples(category):
        X_cat = X[y == category]
        best_bic, best_gmm = np.inf, None
        for n in n_components_range:
            for cov_type in covariance_types:
                gmm = GaussianMixture(n_components=n, covariance_type=cov_type)
                gmm.fit(X_cat)
                bic = gmm.bic(X_cat)
                if bic < best_bic:
                    best_bic, best_gmm = bic, gmm
        n_samples = total_samples // len(np.unique(y))
        X_generated, _ = best_gmm.sample(n_samples)
        return np.round(X_generated, 1), np.full(n_samples, category)

    # G√©n√©ration des √©chantillons pour chaque cat√©gorie
    X_new, y_new = [], []
    for category in np.unique(y):
        X_cat, y_cat = generate_samples(category)
        X_new.append(X_cat)
        y_new.append(y_cat)

    # Concat√©nation des √©chantillons g√©n√©r√©s
    X_new = np.vstack(X_new)
    y_new = np.hstack(y_new)

    # Cr√©ation du DataFrame final
    column_names = list(X.columns) + [target_column]
    df_new = pd.DataFrame(X_new, columns=column_names[:-1])
    df_new[target_column] = y_new

    return df_new


# Initialisation du DataFrame pour stocker les r√©sultats
columns = [
    "It√©ration", "√âchantillonnage", "Mod√®le", "Param√®tres Initiaux",
    "Accuracy Avant", "F1-score Avant", "Param√®tres Optimis√©s",
    "Accuracy Apr√®s", "F1-score Apr√®s"
]
results_df = pd.DataFrame(columns=columns)



# Ex√©cuter 30 it√©rations
for iteration in tqdm(range(1, 31), desc=" It√©rations en cours"):
    print(f"\n Ex√©cution de l'it√©ration {iteration}...\n")

    # Appliquer le sampling si activ√©
    if config["sampling"]["enabled"]:
        sampling_type = config["sampling"].get("sampling_type", "random")

        # Si le sampling type est GMM, appliquer le sampling GMM
        if sampling_type == "gmm":
            df_sampled = gmm_sampling(df, target_column, config)
            sampling_status = "Oui (GMM)"
            print(f" √âchantillonnage GMM activ√©.")
        
        # Si le sampling type est Random, utiliser Pandas sample()
        elif sampling_type == "random":
            fraction = config["sampling"]["fraction"]
            df_sampled = df.sample(frac=fraction)
            sampling_status = "Oui (Random)"
            print(f" √âchantillonnage al√©atoire activ√© : {fraction*100:.1f}% des donn√©es utilis√©es.")
        
        # Si le type est inconnu, lever une erreur
        else:
            raise ValueError(f" Type d'√©chantillonnage inconnu : '{sampling_type}'")

    else:
        # Si le sampling est d√©sactiv√©, utiliser le dataset original
        df_sampled = df
        sampling_status = "Non"
        print(" Aucun √©chantillonnage appliqu√©, utilisation des donn√©es compl√®tes.")

    # S√©paration des donn√©es en train/test
    test_size = config["train_test_split"]["test_size"]
    X = df_sampled.drop(columns=[target_column])
    y = df_sampled[target_column]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)


    # Remplacer X_test et y_test par les donn√©es d'origine
    X_test = df.drop(columns=[target_column])
    y_test = df[target_column]
    
    print(f"\n Donn√©es s√©par√©es : {100 * (1 - test_size):.0f}% entra√Ænement, {100 * test_size:.0f}% test.")

    # Initialiser les mod√®les
    models = {}
    for model_name, model_params in config["models"].items():
        if model_params["enabled"]:
            if model_name == "Decision Tree":
                models[model_name] = DecisionTreeClassifier(max_depth=model_params["max_depth"])
            elif model_name == "Random Forest":
                models[model_name] = RandomForestClassifier(
                    n_estimators=model_params["n_estimators"],
                    max_depth=model_params["max_depth"]
                )
            elif model_name == "SVM":
                models[model_name] = SVC(C=model_params["C"], kernel=model_params["kernel"])
            elif model_name == "Neural Network":
                models[model_name] = MLPClassifier(
                    hidden_layer_sizes=tuple(model_params["hidden_layer_sizes"]),
                    learning_rate_init=model_params["learning_rate_init"],
                    max_iter=500
                )
            elif model_name == "XGBoost":
                models[model_name] = XGBClassifier(
                    n_estimators=model_params["n_estimators"],
                    max_depth=model_params["max_depth"],
                    learning_rate=model_params["learning_rate"]    
                )

    # Entra√Ænement et √©valuation des mod√®les
    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy_before = accuracy_score(y_test, y_pred)
        f1_before = f1_score(y_test, y_pred, average="weighted")
        
        print(f"\n R√©sultats du mod√®le '{model_name}' avant optimisation :")
        print(f"   - Accuracy: {accuracy_before:.4f}")
        print(f"   - F1-score: {f1_before:.4f}")


        best_params = {}
        best_model = model
        accuracy_after, f1_after = "Non optimis√©", "Non optimis√©"

        # Optimisation avec Optuna
        if config["use_optuna"]:
            def objective(trial):
                if model_name == "Decision Tree":
                    max_depth = trial.suggest_int("max_depth", 2, 20)
                    model_opt = DecisionTreeClassifier(max_depth=max_depth)
                elif model_name == "Random Forest":
                    n_estimators = trial.suggest_int("n_estimators", 10, 200)
                    max_depth = trial.suggest_int("max_depth", 2, 20)
                    model_opt = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)
                elif model_name == "SVM":
                    C = trial.suggest_loguniform("C", 0.1, 10)
                    kernel = trial.suggest_categorical("kernel", ["linear", "rbf", "poly"])
                    model_opt = SVC(C=C, kernel=kernel)
                elif model_name == "Neural Network":
                    hidden_layer_sizes = trial.suggest_categorical("hidden_layer_sizes", [(50,), (100,), (50, 50)])
                    learning_rate_init = trial.suggest_loguniform("learning_rate_init", 0.0001, 0.1)
                    model_opt = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=learning_rate_init, max_iter=500)
                elif model_name == "XGBoost":
                    n_estimators = trial.suggest_int("n_estimators", 50, 500)
                    max_depth = trial.suggest_int("max_depth", 2, 20)
                    learning_rate = trial.suggest_loguniform("learning_rate", 0.01, 0.3)
                    model_opt = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate)

                model_opt.fit(X_train, y_train)
                y_pred_opt = model_opt.predict(X_test)
                return accuracy_score(y_test, y_pred_opt)

            study = optuna.create_study(direction="maximize")
            study.optimize(objective, n_trials=config["n_trials"])

            best_params = study.best_trial.params
            print(f"\n Meilleurs param√®tres trouv√©s pour '{model_name}' : {best_params}")

            # R√©entra√Ænement avec les meilleurs param√®tres
            if config["retrain_with_best_params"]:
                best_model = model.__class__(**best_params)
                best_model.fit(X_train, y_train)
                y_pred_opt = best_model.predict(X_test)
                accuracy_after = accuracy_score(y_test, y_pred_opt)
                f1_after = f1_score(y_test, y_pred_opt, average="weighted")
                
            print(f"\n Mod√®le apr√®s optimisation ({model_name}):")
            print(f"   - Accuracy: {accuracy_after}")
            print(f"   - F1-score: {f1_after}")

            # Sauvegarde du meilleur mod√®le
            if config["save_best_model"]:
                model_filename = f"best_model_{model_name}.pkl"
                with open(model_filename, "wb") as f:
                    pickle.dump(best_model, f)
                print(f" Mod√®le '{model_name}' sauvegard√© sous '{model_filename}'.")

        # Stockage des r√©sultats
        results_df = pd.concat([results_df, pd.DataFrame([{
            "It√©ration": iteration,
            "√âchantillonnage": sampling_status,
            "Mod√®le": model_name,
            "Param√®tres Initiaux": {param: value for param, value in model.get_params().items() if param in model_params},
            "Accuracy Avant": accuracy_before,
            "F1-score Avant": f1_before,
            "Param√®tres Optimis√©s": best_params,
            "Accuracy Apr√®s": accuracy_after,
            "F1-score Apr√®s": f1_after
        }])], ignore_index=True)

# Sauvegarde des r√©sultats
# D√©finition du chemin du fichier Excel
excel_path = "resultats_iterations.xlsx"

# V√©rifier si le fichier existe
if os.path.exists(excel_path):
    # Ajouter les nouveaux r√©sultats sans charger les anciens
    with pd.ExcelWriter(excel_path, mode='a', if_sheet_exists='overlay') as writer:
        results_df.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)
else:
    # Si le fichier n'existe pas, on cr√©e un nouveau fichier avec les nouveaux r√©sultats
    results_df.to_excel(excel_path, index=False)

print("\nüìä Les nouveaux r√©sultats ont √©t√© ajout√©s dans 'resultats_iterations.xlsx'.")

# R√©initialisation du DataFrame apr√®s la sauvegarde (conserve les colonnes, supprime les lignes)
results_df.drop(results_df.index, inplace=True)
