{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cc522c8a-18f5-4a09-9d1b-ca25d64c667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import optuna\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c05c0f90-ea4e-456c-8d95-3d47639b6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger la configuration depuis config.json\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "file_path = config[\"file_path\"]\n",
    "random_state = config.get(\"random_state\", 42)\n",
    "model_save_path = config[\"model_save_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "75cc5043-58b2-4eb1-9ff1-8e078d5e58c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Donn√©es charg√©es avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "# Charger le dataset CSV avec gestion des erreurs\n",
    "df = pd.read_csv(file_path)\n",
    "print(\" Donn√©es charg√©es avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c9fab8fe-8d94-49d2-a562-bd03be55a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer des lignes avec la classe Virginica\n",
    "df = df[df['species'].isin(['setosa', 'versicolor'])]\n",
    "df['species'].replace({'versicolor': 0}, inplace=True)\n",
    "df['species'].replace({'setosa': 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e1231222-e9d7-474e-80f1-f0c4fe05c7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Colonne cible d√©finie : species\n"
     ]
    }
   ],
   "source": [
    "# D√©finition de la colonne cible\n",
    "target_column = config[\"target_column\"]\n",
    "if target_column not in df.columns:\n",
    "    raise ValueError(f\" La colonne cible '{target_column}' est absente des donn√©es.\")\n",
    "\n",
    "print(f\" Colonne cible d√©finie : {target_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b51e078b-c4d1-460f-82ff-9102c87cc618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation des param√®tres d√©finis dans config.json\n",
    "def gmm_sampling(df, target_column, config):\n",
    "    n_components_range = config['gmm']['n_components_range']\n",
    "    covariance_types = config['gmm']['covariance_types']\n",
    "    reduction_rate = config['sampling']['fraction']\n",
    "    random_state = config['random_state']\n",
    "\n",
    "    # S√©paration des features et de la cible\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Calcul du nombre total de points √† g√©n√©rer\n",
    "    total_samples = int(len(X) * reduction_rate)\n",
    "\n",
    "    # Fonction pour optimiser le GMM et g√©n√©rer des √©chantillons\n",
    "    def generate_samples(category):\n",
    "        X_cat = X[y == category]\n",
    "        best_bic, best_gmm = np.inf, None\n",
    "        for n in n_components_range:\n",
    "            for cov_type in covariance_types:\n",
    "                gmm = GaussianMixture(n_components=n, covariance_type=cov_type, random_state=random_state)\n",
    "                gmm.fit(X_cat)\n",
    "                bic = gmm.bic(X_cat)\n",
    "                if bic < best_bic:\n",
    "                    best_bic, best_gmm = bic, gmm\n",
    "        n_samples = total_samples // len(np.unique(y))\n",
    "        X_generated, _ = best_gmm.sample(n_samples)\n",
    "        return np.round(X_generated, 1), np.full(n_samples, category)\n",
    "\n",
    "    # G√©n√©ration des √©chantillons pour chaque cat√©gorie\n",
    "    X_new, y_new = [], []\n",
    "    for category in np.unique(y):\n",
    "        X_cat, y_cat = generate_samples(category)\n",
    "        X_new.append(X_cat)\n",
    "        y_new.append(y_cat)\n",
    "\n",
    "    # Concat√©nation des √©chantillons g√©n√©r√©s\n",
    "    X_new = np.vstack(X_new)\n",
    "    y_new = np.hstack(y_new)\n",
    "\n",
    "    # Cr√©ation du DataFrame final\n",
    "    column_names = list(X.columns) + [target_column]\n",
    "    df_new = pd.DataFrame(X_new, columns=column_names[:-1])\n",
    "    df_new[target_column] = y_new\n",
    "\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f4a2e8e6-fcb4-41f5-8808-756c6d7af6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du DataFrame pour stocker les r√©sultats\n",
    "columns = [\n",
    "    \"It√©ration\", \"√âchantillonnage\", \"Mod√®le\", \"Param√®tres Initiaux\",\n",
    "    \"Accuracy Avant\", \"F1-score Avant\", \"Param√®tres Optimis√©s\",\n",
    "    \"Accuracy Apr√®s\", \"F1-score Apr√®s\"\n",
    "]\n",
    "results_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0d8e5084-fa9b-4dfc-949d-787165a39c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher_donnees(df_sampled, target_column, sampling_type, iteration):\n",
    "    print(f\"\\nüìä Donn√©es g√©n√©r√©es ({sampling_type}) - It√©ration {iteration} :\")\n",
    "    print(f\" - Nombre total d'√©chantillons : {df_sampled.shape[0]}\")\n",
    "    print(f\" - Nombre de features : {df_sampled.shape[1] - 1}\")\n",
    "    print(f\" - R√©partition des cat√©gories :\")\n",
    "    print(df_sampled[target_column].value_counts())\n",
    "\n",
    "    print(\"\\nüìù Aper√ßu des 5 premi√®res lignes :\")\n",
    "    print(df_sampled.head(5))\n",
    "\n",
    "    print(\"\\nüìë Affichage complet des donn√©es g√©n√©r√©es :\")\n",
    "    print(df_sampled.to_string(index=False))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "262b6bda-703f-4117-ba9b-2434124e4423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " It√©rations en cours:   0%|          | 0/2 [00:00<?, ?it/s]C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ex√©cution de l'it√©ration 1...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "[I 2025-02-24 16:24:25,783] A new study created in memory with name: no-name-8741e65b-edaa-40aa-9007-aecfca271903\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:25,892] Trial 0 finished with value: 0.82 and parameters: {'n_estimators': 278, 'max_depth': 9, 'learning_rate': 0.22178740848262385}. Best is trial 0 with value: 0.82.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " √âchantillonnage GMM activ√©.\n",
      "\n",
      "üìä Donn√©es g√©n√©r√©es (gmm) - It√©ration 1 :\n",
      " - Nombre total d'√©chantillons : 20\n",
      " - Nombre de features : 4\n",
      " - R√©partition des cat√©gories :\n",
      "species\n",
      "0    10\n",
      "1    10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìù Aper√ßu des 5 premi√®res lignes :\n",
      "   sepal_length  sepal_width  petal_length  petal_width  species\n",
      "0           5.7          2.8           3.9          1.4        0\n",
      "1           6.1          3.1           4.1          1.4        0\n",
      "2           6.2          2.7           4.5          1.3        0\n",
      "3           5.4          2.7           4.6          1.4        0\n",
      "4           6.4          2.8           4.8          1.3        0\n",
      "\n",
      "üìë Affichage complet des donn√©es g√©n√©r√©es :\n",
      " sepal_length  sepal_width  petal_length  petal_width  species\n",
      "          5.7          2.8           3.9          1.4        0\n",
      "          6.1          3.1           4.1          1.4        0\n",
      "          6.2          2.7           4.5          1.3        0\n",
      "          5.4          2.7           4.6          1.4        0\n",
      "          6.4          2.8           4.8          1.3        0\n",
      "          5.2          2.5           3.7          1.0        0\n",
      "          6.1          2.7           4.6          1.4        0\n",
      "          6.2          2.8           4.6          1.6        0\n",
      "          5.8          3.1           4.3          1.3        0\n",
      "          5.4          2.8           4.5          1.5        0\n",
      "          4.9          3.2           1.3          0.3        1\n",
      "          5.2          3.5           1.2          0.3        1\n",
      "          5.2          3.5           1.6          0.2        1\n",
      "          4.6          3.6           1.5          0.2        1\n",
      "          5.3          3.8           1.7          0.2        1\n",
      "          4.5          2.9           1.4          0.1        1\n",
      "          5.1          3.6           1.6          0.3        1\n",
      "          5.1          3.7           1.5          0.5        1\n",
      "          4.9          3.5           1.3          0.1        1\n",
      "          4.6          3.6           1.4          0.3        1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Donn√©es s√©par√©es : 80% entra√Ænement, 20% test.\n",
      "\n",
      " R√©sultats du mod√®le 'XGBoost' avant optimisation :\n",
      "   - Accuracy: 0.8200\n",
      "   - F1-score: 0.8188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,003] Trial 1 finished with value: 0.82 and parameters: {'n_estimators': 294, 'max_depth': 14, 'learning_rate': 0.02099246795966814}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,118] Trial 2 finished with value: 0.82 and parameters: {'n_estimators': 324, 'max_depth': 4, 'learning_rate': 0.023600471812874384}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,252] Trial 3 finished with value: 0.82 and parameters: {'n_estimators': 348, 'max_depth': 11, 'learning_rate': 0.031548872753128794}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,301] Trial 4 finished with value: 0.82 and parameters: {'n_estimators': 82, 'max_depth': 6, 'learning_rate': 0.01880007452569004}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,383] Trial 5 finished with value: 0.82 and parameters: {'n_estimators': 232, 'max_depth': 14, 'learning_rate': 0.16562380848847671}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,443] Trial 6 finished with value: 0.82 and parameters: {'n_estimators': 146, 'max_depth': 19, 'learning_rate': 0.06961473733425275}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,578] Trial 7 finished with value: 0.82 and parameters: {'n_estimators': 413, 'max_depth': 4, 'learning_rate': 0.016702992151850074}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,671] Trial 8 finished with value: 0.82 and parameters: {'n_estimators': 282, 'max_depth': 12, 'learning_rate': 0.03877110031894585}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,822] Trial 9 finished with value: 0.82 and parameters: {'n_estimators': 408, 'max_depth': 11, 'learning_rate': 0.011626401718556195}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:26,927] Trial 10 finished with value: 0.82 and parameters: {'n_estimators': 176, 'max_depth': 8, 'learning_rate': 0.2944639398206448}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:27,045] Trial 11 finished with value: 0.82 and parameters: {'n_estimators': 239, 'max_depth': 16, 'learning_rate': 0.08449145078405108}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:27,215] Trial 12 finished with value: 0.82 and parameters: {'n_estimators': 470, 'max_depth': 8, 'learning_rate': 0.1389254609936849}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:27,348] Trial 13 finished with value: 0.82 and parameters: {'n_estimators': 313, 'max_depth': 17, 'learning_rate': 0.29891888608572353}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:27,455] Trial 14 finished with value: 0.82 and parameters: {'n_estimators': 190, 'max_depth': 14, 'learning_rate': 0.04545400023506047}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:27,601] Trial 15 finished with value: 0.82 and parameters: {'n_estimators': 360, 'max_depth': 8, 'learning_rate': 0.12130756302977944}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:27,739] Trial 16 finished with value: 0.82 and parameters: {'n_estimators': 252, 'max_depth': 2, 'learning_rate': 0.010947174895876327}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:27,832] Trial 17 finished with value: 0.82 and parameters: {'n_estimators': 78, 'max_depth': 14, 'learning_rate': 0.06879031013899287}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:28,008] Trial 18 finished with value: 0.82 and parameters: {'n_estimators': 388, 'max_depth': 10, 'learning_rate': 0.20113727761426353}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:28,416] Trial 19 finished with value: 0.82 and parameters: {'n_estimators': 490, 'max_depth': 19, 'learning_rate': 0.027812349952080744}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:28,574] Trial 20 finished with value: 0.82 and parameters: {'n_estimators': 292, 'max_depth': 13, 'learning_rate': 0.051042354453554394}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:28,733] Trial 21 finished with value: 0.82 and parameters: {'n_estimators': 326, 'max_depth': 5, 'learning_rate': 0.022197030272443557}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:28,878] Trial 22 finished with value: 0.82 and parameters: {'n_estimators': 208, 'max_depth': 2, 'learning_rate': 0.01475792462035053}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:29,025] Trial 23 finished with value: 0.82 and parameters: {'n_estimators': 278, 'max_depth': 16, 'learning_rate': 0.025284295372451433}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:29,218] Trial 24 finished with value: 0.82 and parameters: {'n_estimators': 444, 'max_depth': 6, 'learning_rate': 0.03415446668996411}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:29,380] Trial 25 finished with value: 0.82 and parameters: {'n_estimators': 357, 'max_depth': 9, 'learning_rate': 0.10347562729443088}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:29,486] Trial 26 finished with value: 0.82 and parameters: {'n_estimators': 129, 'max_depth': 3, 'learning_rate': 0.020653334786788036}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:29,636] Trial 27 finished with value: 0.82 and parameters: {'n_estimators': 248, 'max_depth': 6, 'learning_rate': 0.014769357564943943}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:29,803] Trial 28 finished with value: 0.82 and parameters: {'n_estimators': 313, 'max_depth': 10, 'learning_rate': 0.04223612986123428}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:29,992] Trial 29 finished with value: 0.82 and parameters: {'n_estimators': 337, 'max_depth': 17, 'learning_rate': 0.0305557387857155}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:30,164] Trial 30 finished with value: 0.82 and parameters: {'n_estimators': 381, 'max_depth': 12, 'learning_rate': 0.0562301553900493}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:30,335] Trial 31 finished with value: 0.82 and parameters: {'n_estimators': 345, 'max_depth': 7, 'learning_rate': 0.01956208506191054}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:30,497] Trial 32 finished with value: 0.82 and parameters: {'n_estimators': 299, 'max_depth': 11, 'learning_rate': 0.024977078304009488}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:30,640] Trial 33 finished with value: 0.82 and parameters: {'n_estimators': 264, 'max_depth': 13, 'learning_rate': 0.03351182640182863}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:30,787] Trial 34 finished with value: 0.82 and parameters: {'n_estimators': 222, 'max_depth': 5, 'learning_rate': 0.016783231540312}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:30,961] Trial 35 finished with value: 0.82 and parameters: {'n_estimators': 371, 'max_depth': 15, 'learning_rate': 0.19497638814140852}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:31,152] Trial 36 finished with value: 0.82 and parameters: {'n_estimators': 418, 'max_depth': 10, 'learning_rate': 0.013409474509705073}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:31,271] Trial 37 finished with value: 0.82 and parameters: {'n_estimators': 148, 'max_depth': 11, 'learning_rate': 0.03587786328662048}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:31,421] Trial 38 finished with value: 0.82 and parameters: {'n_estimators': 274, 'max_depth': 13, 'learning_rate': 0.06351020371439968}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:31,597] Trial 39 finished with value: 0.82 and parameters: {'n_estimators': 407, 'max_depth': 20, 'learning_rate': 0.08461860664507298}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:31,754] Trial 40 finished with value: 0.82 and parameters: {'n_estimators': 317, 'max_depth': 9, 'learning_rate': 0.02257829761545364}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:31,898] Trial 41 finished with value: 0.82 and parameters: {'n_estimators': 206, 'max_depth': 4, 'learning_rate': 0.01797843231071085}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:32,004] Trial 42 finished with value: 0.82 and parameters: {'n_estimators': 76, 'max_depth': 7, 'learning_rate': 0.012555939587470992}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:32,122] Trial 43 finished with value: 0.82 and parameters: {'n_estimators': 112, 'max_depth': 5, 'learning_rate': 0.029160119071140217}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:32,249] Trial 44 finished with value: 0.82 and parameters: {'n_estimators': 168, 'max_depth': 3, 'learning_rate': 0.017315607384067244}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:32,407] Trial 45 finished with value: 0.82 and parameters: {'n_estimators': 301, 'max_depth': 7, 'learning_rate': 0.042301890211214374}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:32,492] Trial 46 finished with value: 0.82 and parameters: {'n_estimators': 53, 'max_depth': 9, 'learning_rate': 0.25757519424408765}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:32,674] Trial 47 finished with value: 0.82 and parameters: {'n_estimators': 339, 'max_depth': 4, 'learning_rate': 0.010495130157740694}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:32,813] Trial 48 finished with value: 0.82 and parameters: {'n_estimators': 222, 'max_depth': 12, 'learning_rate': 0.02389164800126609}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:32,967] Trial 49 finished with value: 0.82 and parameters: {'n_estimators': 248, 'max_depth': 8, 'learning_rate': 0.01957871965574258}. Best is trial 0 with value: 0.82.\n",
      " It√©rations en cours:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:12<00:12, 12.29s/it]C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Meilleurs param√®tres trouv√©s pour 'XGBoost' : {'n_estimators': 278, 'max_depth': 9, 'learning_rate': 0.22178740848262385}\n",
      "\n",
      " Mod√®le apr√®s optimisation (XGBoost):\n",
      "   - Accuracy: 0.82\n",
      "   - F1-score: 0.818840579710145\n",
      " Mod√®le 'XGBoost' sauvegard√© sous 'best_model_XGBoost.pkl'.\n",
      "\n",
      " Ex√©cution de l'it√©ration 2...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yando\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "[I 2025-02-24 16:24:39,937] A new study created in memory with name: no-name-acad156e-5005-429c-ad4c-78118588beed\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,005] Trial 0 finished with value: 0.82 and parameters: {'n_estimators': 139, 'max_depth': 12, 'learning_rate': 0.05761694098740558}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,070] Trial 1 finished with value: 0.82 and parameters: {'n_estimators': 132, 'max_depth': 4, 'learning_rate': 0.18608237375835676}. Best is trial 0 with value: 0.82.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " √âchantillonnage GMM activ√©.\n",
      "\n",
      "üìä Donn√©es g√©n√©r√©es (gmm) - It√©ration 2 :\n",
      " - Nombre total d'√©chantillons : 20\n",
      " - Nombre de features : 4\n",
      " - R√©partition des cat√©gories :\n",
      "species\n",
      "0    10\n",
      "1    10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìù Aper√ßu des 5 premi√®res lignes :\n",
      "   sepal_length  sepal_width  petal_length  petal_width  species\n",
      "0           5.7          2.8           3.9          1.4        0\n",
      "1           6.1          3.1           4.1          1.4        0\n",
      "2           6.2          2.7           4.5          1.3        0\n",
      "3           5.4          2.7           4.6          1.4        0\n",
      "4           6.4          2.8           4.8          1.3        0\n",
      "\n",
      "üìë Affichage complet des donn√©es g√©n√©r√©es :\n",
      " sepal_length  sepal_width  petal_length  petal_width  species\n",
      "          5.7          2.8           3.9          1.4        0\n",
      "          6.1          3.1           4.1          1.4        0\n",
      "          6.2          2.7           4.5          1.3        0\n",
      "          5.4          2.7           4.6          1.4        0\n",
      "          6.4          2.8           4.8          1.3        0\n",
      "          5.2          2.5           3.7          1.0        0\n",
      "          6.1          2.7           4.6          1.4        0\n",
      "          6.2          2.8           4.6          1.6        0\n",
      "          5.8          3.1           4.3          1.3        0\n",
      "          5.4          2.8           4.5          1.5        0\n",
      "          4.9          3.2           1.3          0.3        1\n",
      "          5.2          3.5           1.2          0.3        1\n",
      "          5.2          3.5           1.6          0.2        1\n",
      "          4.6          3.6           1.5          0.2        1\n",
      "          5.3          3.8           1.7          0.2        1\n",
      "          4.5          2.9           1.4          0.1        1\n",
      "          5.1          3.6           1.6          0.3        1\n",
      "          5.1          3.7           1.5          0.5        1\n",
      "          4.9          3.5           1.3          0.1        1\n",
      "          4.6          3.6           1.4          0.3        1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Donn√©es s√©par√©es : 80% entra√Ænement, 20% test.\n",
      "\n",
      " R√©sultats du mod√®le 'XGBoost' avant optimisation :\n",
      "   - Accuracy: 0.8200\n",
      "   - F1-score: 0.8188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,152] Trial 2 finished with value: 0.82 and parameters: {'n_estimators': 136, 'max_depth': 4, 'learning_rate': 0.014230487774999016}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,276] Trial 3 finished with value: 0.82 and parameters: {'n_estimators': 279, 'max_depth': 10, 'learning_rate': 0.010246013220039046}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,327] Trial 4 finished with value: 0.82 and parameters: {'n_estimators': 129, 'max_depth': 12, 'learning_rate': 0.15145385609656709}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,473] Trial 5 finished with value: 0.82 and parameters: {'n_estimators': 469, 'max_depth': 4, 'learning_rate': 0.019043443495751825}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,592] Trial 6 finished with value: 0.82 and parameters: {'n_estimators': 352, 'max_depth': 16, 'learning_rate': 0.021293432966677295}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,696] Trial 7 finished with value: 0.82 and parameters: {'n_estimators': 287, 'max_depth': 13, 'learning_rate': 0.01117289699319665}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,795] Trial 8 finished with value: 0.82 and parameters: {'n_estimators': 268, 'max_depth': 9, 'learning_rate': 0.2989472447187047}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:40,935] Trial 9 finished with value: 0.82 and parameters: {'n_estimators': 408, 'max_depth': 14, 'learning_rate': 0.015586995544725845}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:41,169] Trial 10 finished with value: 0.82 and parameters: {'n_estimators': 62, 'max_depth': 20, 'learning_rate': 0.05041490376771199}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:41,295] Trial 11 finished with value: 0.82 and parameters: {'n_estimators': 178, 'max_depth': 7, 'learning_rate': 0.08040865591713775}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:41,404] Trial 12 finished with value: 0.82 and parameters: {'n_estimators': 214, 'max_depth': 7, 'learning_rate': 0.05884364733036358}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:41,477] Trial 13 finished with value: 0.82 and parameters: {'n_estimators': 60, 'max_depth': 2, 'learning_rate': 0.1399731388780959}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:41,572] Trial 14 finished with value: 0.82 and parameters: {'n_estimators': 129, 'max_depth': 17, 'learning_rate': 0.03336326618843628}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:41,684] Trial 15 finished with value: 0.82 and parameters: {'n_estimators': 216, 'max_depth': 8, 'learning_rate': 0.29694591674348236}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:41,772] Trial 16 finished with value: 0.82 and parameters: {'n_estimators': 97, 'max_depth': 5, 'learning_rate': 0.12222067997475089}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:41,889] Trial 17 finished with value: 0.82 and parameters: {'n_estimators': 180, 'max_depth': 11, 'learning_rate': 0.19380699266592596}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,033] Trial 18 finished with value: 0.82 and parameters: {'n_estimators': 335, 'max_depth': 2, 'learning_rate': 0.08467719650955216}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,155] Trial 19 finished with value: 0.82 and parameters: {'n_estimators': 218, 'max_depth': 15, 'learning_rate': 0.03460674680550422}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,260] Trial 20 finished with value: 0.82 and parameters: {'n_estimators': 173, 'max_depth': 18, 'learning_rate': 0.20695506924250995}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,352] Trial 21 finished with value: 0.82 and parameters: {'n_estimators': 122, 'max_depth': 5, 'learning_rate': 0.032907732750122945}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,434] Trial 22 finished with value: 0.82 and parameters: {'n_estimators': 92, 'max_depth': 5, 'learning_rate': 0.0958272991041785}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,530] Trial 23 finished with value: 0.82 and parameters: {'n_estimators': 155, 'max_depth': 3, 'learning_rate': 0.05065893436871921}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,606] Trial 24 finished with value: 0.82 and parameters: {'n_estimators': 50, 'max_depth': 7, 'learning_rate': 0.06397476047738122}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,731] Trial 25 finished with value: 0.82 and parameters: {'n_estimators': 244, 'max_depth': 11, 'learning_rate': 0.02279050299225792}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,833] Trial 26 finished with value: 0.82 and parameters: {'n_estimators': 98, 'max_depth': 9, 'learning_rate': 0.037645054520559204}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:42,949] Trial 27 finished with value: 0.82 and parameters: {'n_estimators': 154, 'max_depth': 6, 'learning_rate': 0.01543347443709263}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:43,068] Trial 28 finished with value: 0.82 and parameters: {'n_estimators': 201, 'max_depth': 13, 'learning_rate': 0.024878883486038315}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:43,206] Trial 29 finished with value: 0.82 and parameters: {'n_estimators': 295, 'max_depth': 10, 'learning_rate': 0.010627725252499864}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:43,326] Trial 30 finished with value: 0.82 and parameters: {'n_estimators': 248, 'max_depth': 3, 'learning_rate': 0.2174618939876697}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:43,483] Trial 31 finished with value: 0.82 and parameters: {'n_estimators': 315, 'max_depth': 12, 'learning_rate': 0.013692351392849385}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:43,646] Trial 32 finished with value: 0.82 and parameters: {'n_estimators': 372, 'max_depth': 4, 'learning_rate': 0.013515616449190566}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:43,762] Trial 33 finished with value: 0.82 and parameters: {'n_estimators': 132, 'max_depth': 10, 'learning_rate': 0.010139567897953412}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:43,968] Trial 34 finished with value: 0.82 and parameters: {'n_estimators': 483, 'max_depth': 13, 'learning_rate': 0.018326063350931992}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:44,141] Trial 35 finished with value: 0.82 and parameters: {'n_estimators': 433, 'max_depth': 15, 'learning_rate': 0.02729651547913631}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:44,271] Trial 36 finished with value: 0.82 and parameters: {'n_estimators': 243, 'max_depth': 9, 'learning_rate': 0.018329917346544586}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:44,399] Trial 37 finished with value: 0.82 and parameters: {'n_estimators': 274, 'max_depth': 6, 'learning_rate': 0.10953376918196074}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:44,500] Trial 38 finished with value: 0.82 and parameters: {'n_estimators': 82, 'max_depth': 3, 'learning_rate': 0.012257621903081613}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:44,615] Trial 39 finished with value: 0.82 and parameters: {'n_estimators': 149, 'max_depth': 12, 'learning_rate': 0.04183441522531872}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:44,785] Trial 40 finished with value: 0.82 and parameters: {'n_estimators': 370, 'max_depth': 14, 'learning_rate': 0.07573211712000806}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:44,893] Trial 41 finished with value: 0.82 and parameters: {'n_estimators': 118, 'max_depth': 12, 'learning_rate': 0.17162872340581595}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:45,020] Trial 42 finished with value: 0.82 and parameters: {'n_estimators': 194, 'max_depth': 8, 'learning_rate': 0.14222162932527208}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:45,116] Trial 43 finished with value: 0.82 and parameters: {'n_estimators': 112, 'max_depth': 16, 'learning_rate': 0.2738463727389104}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:45,226] Trial 44 finished with value: 0.82 and parameters: {'n_estimators': 154, 'max_depth': 20, 'learning_rate': 0.15876579038397898}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:45,321] Trial 45 finished with value: 0.82 and parameters: {'n_estimators': 77, 'max_depth': 11, 'learning_rate': 0.2535189789076701}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:45,433] Trial 46 finished with value: 0.82 and parameters: {'n_estimators': 137, 'max_depth': 14, 'learning_rate': 0.06592888465739631}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:45,551] Trial 47 finished with value: 0.82 and parameters: {'n_estimators': 174, 'max_depth': 10, 'learning_rate': 0.12041184072734602}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:45,702] Trial 48 finished with value: 0.82 and parameters: {'n_estimators': 294, 'max_depth': 8, 'learning_rate': 0.04347722239095782}. Best is trial 0 with value: 0.82.\n",
      "C:\\Users\\yando\\AppData\\Local\\Temp\\ipykernel_27068\\4015627347.py:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
      "[I 2025-02-24 16:24:45,801] Trial 49 finished with value: 0.82 and parameters: {'n_estimators': 69, 'max_depth': 4, 'learning_rate': 0.23829209723670355}. Best is trial 0 with value: 0.82.\n",
      " It√©rations en cours: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:25<00:00, 12.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Meilleurs param√®tres trouv√©s pour 'XGBoost' : {'n_estimators': 139, 'max_depth': 12, 'learning_rate': 0.05761694098740558}\n",
      "\n",
      " Mod√®le apr√®s optimisation (XGBoost):\n",
      "   - Accuracy: 0.82\n",
      "   - F1-score: 0.818840579710145\n",
      " Mod√®le 'XGBoost' sauvegard√© sous 'best_model_XGBoost.pkl'.\n",
      "\n",
      "üìä Les nouveaux r√©sultats ont √©t√© ajout√©s dans 'resultats_iterations.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ex√©cuter 30 it√©rations\n",
    "for iteration in tqdm(range(1, 3), desc=\" It√©rations en cours\"):\n",
    "    print(f\"\\n Ex√©cution de l'it√©ration {iteration}...\\n\")\n",
    "\n",
    "    # Appliquer le sampling si activ√©\n",
    "    if config[\"sampling\"][\"enabled\"]:\n",
    "        sampling_type = config[\"sampling\"].get(\"sampling_type\", \"random\")\n",
    "\n",
    "        # Si le sampling type est GMM, appliquer le sampling GMM\n",
    "        if sampling_type == \"gmm\":\n",
    "            df_sampled = gmm_sampling(df, target_column, config)\n",
    "            sampling_status = \"Oui (GMM)\"\n",
    "            print(f\" √âchantillonnage GMM activ√©.\")\n",
    "        \n",
    "        # Si le sampling type est Random, utiliser Pandas sample()\n",
    "        elif sampling_type == \"random\":\n",
    "            fraction = config[\"sampling\"][\"fraction\"]\n",
    "            df_sampled = df.sample(frac=fraction)\n",
    "            sampling_status = \"Oui (Random)\"\n",
    "            print(f\" √âchantillonnage al√©atoire activ√© : {fraction*100:.1f}% des donn√©es utilis√©es.\")\n",
    "        \n",
    "        # Si le type est inconnu, lever une erreur\n",
    "        else:\n",
    "            raise ValueError(f\" Type d'√©chantillonnage inconnu : '{sampling_type}'\")\n",
    "\n",
    "        #  Appel de la fonction d'affichage des donn√©es g√©n√©r√©es \n",
    "        afficher_donnees(df_sampled, target_column, sampling_type, iteration)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Si le sampling est d√©sactiv√©, utiliser le dataset original\n",
    "        df_sampled = df\n",
    "        sampling_status = \"Non\"\n",
    "        print(\" Aucun √©chantillonnage appliqu√©, utilisation des donn√©es compl√®tes.\")\n",
    "\n",
    "    # S√©paration des donn√©es en train/test\n",
    "    test_size = config[\"train_test_split\"][\"test_size\"]\n",
    "    X = df_sampled.drop(columns=[target_column])\n",
    "    y = df_sampled[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Remplacer X_test et y_test par les donn√©es d'origine\n",
    "    X_test = df.drop(columns=[target_column])\n",
    "    y_test = df[target_column]\n",
    "\n",
    "    print(f\"\\n Donn√©es s√©par√©es : {100 * (1 - test_size):.0f}% entra√Ænement, {100 * test_size:.0f}% test.\")\n",
    "\n",
    "    # Initialiser les mod√®les\n",
    "    models = {}\n",
    "    for model_name, model_params in config[\"models\"].items():\n",
    "        if model_params[\"enabled\"]:\n",
    "            if model_name == \"Decision Tree\":\n",
    "                models[model_name] = DecisionTreeClassifier(max_depth=model_params[\"max_depth\"], random_state=random_state)\n",
    "            elif model_name == \"Random Forest\":\n",
    "                models[model_name] = RandomForestClassifier(\n",
    "                    n_estimators=model_params[\"n_estimators\"],\n",
    "                    max_depth=model_params[\"max_depth\"],\n",
    "                    random_state=random_state\n",
    "                )\n",
    "            elif model_name == \"SVM\":\n",
    "                models[model_name] = SVC(C=model_params[\"C\"], kernel=model_params[\"kernel\"])\n",
    "            elif model_name == \"Neural Network\":\n",
    "                models[model_name] = MLPClassifier(\n",
    "                    hidden_layer_sizes=tuple(model_params[\"hidden_layer_sizes\"]),\n",
    "                    learning_rate_init=model_params[\"learning_rate_init\"],\n",
    "                    max_iter=500,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "            elif model_name == \"XGBoost\":\n",
    "                models[model_name] = XGBClassifier(\n",
    "                    n_estimators=model_params[\"n_estimators\"],\n",
    "                    max_depth=model_params[\"max_depth\"],\n",
    "                    learning_rate=model_params[\"learning_rate\"],\n",
    "                    random_state=random_state    \n",
    "                )\n",
    "\n",
    "    # Entra√Ænement et √©valuation des mod√®les\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy_before = accuracy_score(y_test, y_pred)\n",
    "        f1_before = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        \n",
    "        print(f\"\\n R√©sultats du mod√®le '{model_name}' avant optimisation :\")\n",
    "        print(f\"   - Accuracy: {accuracy_before:.4f}\")\n",
    "        print(f\"   - F1-score: {f1_before:.4f}\")\n",
    "\n",
    "\n",
    "        best_params = {}\n",
    "        best_model = model\n",
    "        accuracy_after, f1_after = \"Non optimis√©\", \"Non optimis√©\"\n",
    "\n",
    "        # Optimisation avec Optuna\n",
    "        if config[\"use_optuna\"]:\n",
    "            def objective(trial):\n",
    "                if model_name == \"Decision Tree\":\n",
    "                    max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n",
    "                    model_opt = DecisionTreeClassifier(max_depth=max_depth, random_state=random_state)\n",
    "                elif model_name == \"Random Forest\":\n",
    "                    n_estimators = trial.suggest_int(\"n_estimators\", 10, 200)\n",
    "                    max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n",
    "                    model_opt = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "                elif model_name == \"SVM\":\n",
    "                    C = trial.suggest_loguniform(\"C\", 0.1, 10)\n",
    "                    kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\"])\n",
    "                    model_opt = SVC(C=C, kernel=kernel)\n",
    "                elif model_name == \"Neural Network\":\n",
    "                    hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\", [(50,), (100,), (50, 50)])\n",
    "                    learning_rate_init = trial.suggest_loguniform(\"learning_rate_init\", 0.0001, 0.1)\n",
    "                    model_opt = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=learning_rate_init, max_iter=500, random_state=random_state)\n",
    "                elif model_name == \"XGBoost\":\n",
    "                    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "                    max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n",
    "                    learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3)\n",
    "                    model_opt = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, random_state=random_state)\n",
    "\n",
    "                model_opt.fit(X_train, y_train)\n",
    "            \n",
    "                y_pred_opt = model_opt.predict(X_test)\n",
    "                return accuracy_score(y_test, y_pred_opt)\n",
    "\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials=config[\"n_trials\"])\n",
    "\n",
    "            best_params = study.best_trial.params\n",
    "            print(f\"\\n Meilleurs param√®tres trouv√©s pour '{model_name}' : {best_params}\")\n",
    "\n",
    "            # R√©entra√Ænement avec les meilleurs param√®tres\n",
    "            if config[\"retrain_with_best_params\"]:\n",
    "                best_model = model.__class__(**best_params, random_state=random_state)\n",
    "                best_model.fit(X_train, y_train)\n",
    "                y_pred_opt = best_model.predict(X_test)\n",
    "                accuracy_after = accuracy_score(y_test, y_pred_opt)\n",
    "                f1_after = f1_score(y_test, y_pred_opt, average=\"weighted\")\n",
    "                \n",
    "            print(f\"\\n Mod√®le apr√®s optimisation ({model_name}):\")\n",
    "            print(f\"   - Accuracy: {accuracy_after}\")\n",
    "            print(f\"   - F1-score: {f1_after}\")\n",
    "\n",
    "            # Sauvegarde du meilleur mod√®le\n",
    "            if config[\"save_best_model\"]:\n",
    "                model_filename = f\"best_model_{model_name}.pkl\"\n",
    "                with open(model_filename, \"wb\") as f:\n",
    "                    pickle.dump(best_model, f)\n",
    "                print(f\" Mod√®le '{model_name}' sauvegard√© sous '{model_filename}'.\")\n",
    "\n",
    "        # Stockage des r√©sultats\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "            \"It√©ration\": iteration,\n",
    "            \"√âchantillonnage\": sampling_status,\n",
    "            \"Mod√®le\": model_name,\n",
    "            \"Param√®tres Initiaux\": {param: value for param, value in model.get_params().items() if param in model_params},\n",
    "            \"Accuracy Avant\": accuracy_before,\n",
    "            \"F1-score Avant\": f1_before,\n",
    "            \"Param√®tres Optimis√©s\": best_params,\n",
    "            \"Accuracy Apr√®s\": accuracy_after,\n",
    "            \"F1-score Apr√®s\": f1_after\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "# Sauvegarde des r√©sultats\n",
    "# D√©finition du chemin du fichier Excel\n",
    "excel_path = \"resultats_iterations.xlsx\"\n",
    "\n",
    "# V√©rifier si le fichier existe\n",
    "if os.path.exists(excel_path):\n",
    "    # Ajouter les nouveaux r√©sultats sans charger les anciens\n",
    "    with pd.ExcelWriter(excel_path, mode='a', if_sheet_exists='overlay') as writer:\n",
    "        results_df.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "else:\n",
    "    # Si le fichier n'existe pas, on cr√©e un nouveau fichier avec les nouveaux r√©sultats\n",
    "    results_df.to_excel(excel_path, index=False)\n",
    "\n",
    "print(\"\\nüìä Les nouveaux r√©sultats ont √©t√© ajout√©s dans 'resultats_iterations.xlsx'.\")\n",
    "\n",
    "# R√©initialisation du DataFrame apr√®s la sauvegarde (conserve les colonnes, supprime les lignes)\n",
    "results_df.drop(results_df.index, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994e8a3-b9e8-4c37-bdc5-3c8b2302be05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
